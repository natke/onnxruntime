{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
        "Licensed under the MIT License.\n",
        "\n",
        "# Inference Bert Model for High Performance with ONNX Runtime on AzureML #\n",
        "\n",
        "This tutorial takes a pre-trained BERT model, converts it to ONNX, and deploys the ONNX model with ONNX Runtime through AzureML.\n",
        "In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
        "\n",
        "## Contents\n",
        "\n",
        "**Prerequisites** to set up your Azure ML work environments\n",
        "\n",
        "**Obtain model and convert to ONNX**\n",
        "\n",
        "**Deploy Bert model using ONNX Runtime and AzureML**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "\n",
        "To run on AzureML, you need:\n",
        "* Azure subscription\n",
        "* Azure Machine Learning Workspace\n",
        "* the Azure Machine Learning SDK\n",
        "\n",
        "You might also find the following resources useful:\n",
        "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
        "* The [Azure Portal](https://portal.azure.com) allows you to track the status of your deployments."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# To install dependencies directly run the following\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install azureml azureml.core\n",
        "!pip install onnxruntime\n",
        "!pip install matplotlib\n",
        "\n",
        "# To create a a Jupter kernel from your conda environment, run the following. replacing <kernel name> with your own name\n",
        "#   conda install -c anaconda ipykernel\n",
        "#   python -m ipykernel install --user --name=<kernel name>"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.9.1)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torch) (0.8)\nRequirement already satisfied: typing-extensions in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torch) (3.10.0.2)\nRequirement already satisfied: transformers in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (4.5.1)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (4.62.2)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.8)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (4.8.1)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: sacremoses in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.0.45)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (2021.8.28)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (21.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (8.0.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: azureml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.2.7)\nCollecting azureml.core\n  Using cached azureml_core-1.40.0-py3-none-any.whl (2.7 MB)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml) (2.26.0)\nRequirement already satisfied: python-dateutil in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml) (2.8.2)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml) (0.25.3)\nRequirement already satisfied: contextlib2<22.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (21.6.0)\nRequirement already satisfied: knack~=0.9.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.9.0)\nRequirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.61.1)\nRequirement already satisfied: jmespath<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.10.0)\nRequirement already satisfied: msrest<1.0.0,>=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.6.21)\nRequirement already satisfied: SecretStorage<4.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (3.3.1)\nRequirement already satisfied: packaging<22.0,>=20.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (21.0)\nRequirement already satisfied: azure-common<2.0.0,>=1.1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.1.27)\nRequirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.61.0)\nRequirement already satisfied: humanfriendly<11.0,>=4.7 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (9.2)\nRequirement already satisfied: pytz in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2021.1)\nRequirement already satisfied: argcomplete<2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.12.3)\nRequirement already satisfied: pyopenssl<22.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (20.0.1)\nRequirement already satisfied: urllib3<=1.26.7,>=1.23 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.25.11)\nRequirement already satisfied: backports.tempfile in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.0)\nRequirement already satisfied: azure-mgmt-storage<20.0.0,>=16.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (19.1.0)\nRequirement already satisfied: msal<2.0.0,>=1.15.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.17.0)\nRequirement already satisfied: azure-mgmt-resource<21.0.0,>=15.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (20.1.0)\nRequirement already satisfied: azure-mgmt-keyvault<10.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (9.1.0)\nRequirement already satisfied: pkginfo in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.7.1)\nRequirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.6.4)\nRequirement already satisfied: adal<=1.2.7,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.2.7)\nRequirement already satisfied: paramiko<3.0.0,>=2.0.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2.7.2)\nRequirement already satisfied: pathspec<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.9.0)\nRequirement already satisfied: PyJWT<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2.1.0)\nRequirement already satisfied: ndg-httpsclient<=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.5.1)\nRequirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (3.4.8)\nRequirement already satisfied: msal-extensions<0.4,>=0.3.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.3.1)\nRequirement already satisfied: azure-core<1.22 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.18.0)\nRequirement already satisfied: docker<6.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (5.0.2)\nRequirement already satisfied: jsonpickle<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2.0.0)\nRequirement already satisfied: azure-mgmt-containerregistry<9.0.0,>=8.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (8.2.0)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azureml) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azureml) (3.2)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azureml) (2021.5.30)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from python-dateutil->azureml) (1.16.0)\nRequirement already satisfied: numpy>=1.13.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pandas->azureml) (1.18.5)\nRequirement already satisfied: pygments in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.9.0->azureml.core) (2.10.0)\nRequirement already satisfied: tabulate in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.9.0->azureml.core) (0.8.9)\nRequirement already satisfied: pyyaml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.9.0->azureml.core) (5.4.1)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml.core) (1.3.0)\nRequirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml.core) (0.6.0)\nRequirement already satisfied: jeepney>=0.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from SecretStorage<4.0.0->azureml.core) (0.7.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from packaging<22.0,>=20.0->azureml.core) (2.4.7)\nRequirement already satisfied: importlib-metadata<5,>=0.23; python_version == \"3.6\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from argcomplete<2.0->azureml.core) (4.8.1)\nRequirement already satisfied: backports.weakref in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from backports.tempfile->azureml.core) (1.0.post1)\nRequirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-mgmt-storage<20.0.0,>=16.0.0->azureml.core) (1.3.0)\nRequirement already satisfied: bcrypt>=3.1.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml.core) (3.2.0)\nRequirement already satisfied: pynacl>=1.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml.core) (1.4.0)\nRequirement already satisfied: pyasn1>=0.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ndg-httpsclient<=0.5.1->azureml.core) (0.4.8)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml.core) (1.14.6)\nRequirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msal-extensions<0.4,>=0.3.0->azureml.core) (1.7.1)\nRequirement already satisfied: websocket-client>=0.32.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from docker<6.0.0->azureml.core) (1.2.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml.core) (3.1.1)\nRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata<5,>=0.23; python_version == \"3.6\"->argcomplete<2.0->azureml.core) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata<5,>=0.23; python_version == \"3.6\"->argcomplete<2.0->azureml.core) (3.5.0)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml.core) (2.20)\n\u001b[31mERROR: azureml-widgets 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-train-core 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-train-automl-runtime 1.34.0.post1 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-train-automl-client 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-tensorboard 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-telemetry 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-sdk 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-responsibleai 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-responsibleai 1.34.0 has requirement responsibleai==0.9.4, but you'll have responsibleai 0.7.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-pipeline-core 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-opendatasets 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-mlflow 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-interpret 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-defaults 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-datadrift 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-contrib-services 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-contrib-server 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-contrib-reinforcementlearning 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-contrib-pipeline-steps 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-contrib-notebook 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-contrib-fairness 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-contrib-dataset 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-cli-common 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-automl-dnn-nlp 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-accel-models 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\nInstalling collected packages: azureml.core\nSuccessfully installed azureml.core\nRequirement already satisfied: onnxruntime in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.8.0)\nRequirement already satisfied: flatbuffers in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from onnxruntime) (2.0)\nRequirement already satisfied: protobuf in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from onnxruntime) (3.17.3)\nRequirement already satisfied: numpy>=1.16.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from onnxruntime) (1.18.5)\nRequirement already satisfied: six>=1.9 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf->onnxruntime) (1.16.0)\nRequirement already satisfied: matplotlib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (3.2.1)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: numpy>=1.11 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (1.18.5)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (2.4.7)\nRequirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
        }
      ],
      "execution_count": 101,
      "metadata": {
        "gather": {
          "logged": 1649367997605
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "device_id = 0\n",
        "device = 'cpu'\n",
        "#if torch.cuda.is_available():\n",
        "#    device = 'cuda'\n"
      ],
      "outputs": [],
      "execution_count": 102,
      "metadata": {
        "gather": {
          "logged": 1649367997859
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtain and convert PyTorch model to ONNX format\n",
        "\n",
        "In the code below, we obtain a BERT model fine-tuned for question answering with the SQUAD dataset from HuggingFace.\n",
        "\n",
        "If you'd like to pre-train a BERT model from scratch, follow the instructions in\n",
        "[Pretraining of the BERT model](https://github.com/microsoft/AzureML-BERT/blob/master/pretrain/PyTorch/notebooks/BERT_Pretrain.ipynb). \n",
        "And if you would like to fine-tune the model with your own dataset, refer to  [AzureML Bert Eval Squad](https://github.com/microsoft/AzureML-BERT/blob/master/finetune/PyTorch/notebooks/BERT_Eval_SQUAD.ipynb)\n",
        "or [AzureML Bert Eval GLUE](https://github.com/microsoft/AzureML-BERT/blob/master/finetune/PyTorch/notebooks/BERT_Eval_GLUE.ipynb).\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the tokenizer and model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name).to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2022-04-07 21:46:41.800059801 [E:onnxruntime:Default, cuda_call.cc:118 CudaCall] CUDA failure 700: an illegal memory access was encountered ; GPU=0 ; hostname=nakersha3 ; expr=cudaStreamDestroy(stream_); \n2022-04-07 21:46:41.800437085 [E:onnxruntime:Default, cuda_call.cc:118 CudaCall] CUDNN failure 4: CUDNN_STATUS_INTERNAL_ERROR ; GPU=0 ; hostname=nakersha3 ; expr=cudnnDestroy(cudnn_handle_); \n"
        }
      ],
      "execution_count": 103,
      "metadata": {
        "gather": {
          "logged": 1649368006650
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 1024"
      ],
      "outputs": [],
      "execution_count": 104,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649368006949
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample input and question"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#question = \"What is a major importance of Southern California in relation to California and the United States?\"\n",
        "#context = \"Southern California, often abbreviated SoCal, is a geographic and cultural region that generally comprises California's southernmost 10 counties. The region is traditionally described as \\\"eight counties\\\", based on demographics and economic ties: Imperial, Los Angeles, Orange, Riverside, San Bernardino, San Diego, Santa Barbara, and Ventura. The more extensive 10-county definition, including Kern and San Luis Obispo counties, is also used based on historical political divisions. Southern California is a major economic center for the state of California and the United States.\"\n",
        "question = \"What is my name\"\n",
        "context = \"My name is Natalie and my friend's name is Jane\""
      ],
      "outputs": [],
      "execution_count": 105,
      "metadata": {
        "gather": {
          "logged": 1649368007332
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the PyTorch model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the pre processing code, which encodes the input question and context into token ids and segment ids. This function also returns the list of tokens in the input, used during post processing to map the output of the model back to a phrase."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(question, context) -> tuple:\n",
        "    encoded_input = tokenizer(question, context, return_tensors='pt')\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded_input.input_ids[0].tolist())\n",
        "    return (encoded_input.input_ids, encoded_input.token_type_ids, tokens)"
      ],
      "outputs": [],
      "execution_count": 106,
      "metadata": {
        "gather": {
          "logged": 1649368007684
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the post processing code which takes the start and end token ids from the model output, determines if they are sequential, and maps them back into words to make an answer phrase."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(tokens, output) -> dict:\n",
        "    results = {}\n",
        "    answer_start = torch.argmax(output['start_logits'])\n",
        "    answer_end = torch.argmax(output['end_logits'])\n",
        "    if answer_end >= answer_start:\n",
        "        answer = tokens[answer_start]\n",
        "        for i in range(answer_start+1, answer_end+1):\n",
        "            if tokens[i][0:2] == \"##\":\n",
        "                answer += tokens[i][2:]\n",
        "            else:\n",
        "                answer += \" \" + tokens[i]\n",
        "        results['question'] = question.capitalize()\n",
        "        results['answer'] = answer.capitalize()\n",
        "    else:\n",
        "        results['error'] = \"I am unable to find the answer to this question. Can you please ask another question?\"\n",
        "    return results"
      ],
      "outputs": [],
      "execution_count": 107,
      "metadata": {
        "gather": {
          "logged": 1649368008089
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the PyTorch model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "input_ids, segment_ids, tokens = preprocess(question, context)\n",
        "\n",
        "outputs = model(input_ids.to(device), token_type_ids=segment_ids.to(device)) \n",
        "\n",
        "print(outputs.start_logits.shape)\n",
        "print(outputs.end_logits.shape)\n",
        "\n",
        "results = postprocess(tokens, outputs)\n",
        "results\n",
        " "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([1, 19])\ntorch.Size([1, 19])\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 108,
          "data": {
            "text/plain": "{'question': 'What is my name', 'answer': 'Natalie'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 108,
      "metadata": {
        "gather": {
          "logged": 1649368008636
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 109,
          "data": {
            "text/plain": "{'question': 'What is my name', 'answer': 'Natalie'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 109,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649368009031
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 110,
          "data": {
            "text/plain": "'cpu'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 110,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649368009505
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "output_model_path = \"./\" + model_name + \".onnx\"\n",
        "\n",
        "# set the model to inference mode\n",
        "# It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, \n",
        "# to turn the model to inference mode. This is required since operators like dropout or batchnorm \n",
        "# behave differently in inference and training mode.\n",
        "model.eval()\n",
        "\n",
        "# Generate dummy inputs to the model. Adjust if neccessary\n",
        "inputs = {\n",
        "        'input_ids':   torch.randint(32, [1, 32], dtype=torch.long).to(device), # list of numerical ids for the tokenised text\n",
        "        'token_type_ids':  torch.ones([1, 32], dtype=torch.long).to(device),    # dummy list of ones\n",
        "    }\n",
        "\n",
        "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
        "torch.onnx.export(model,                                         # model being run\n",
        "                  (inputs['input_ids'], \n",
        "                   inputs['token_type_ids']),                    # model input (or a tuple for multiple inputs)\n",
        "                  output_model_path,                             # where to save the model (can be a file or file-like object)\n",
        "                  opset_version=11,                              # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,                      # whether to execute constant folding for optimization\n",
        "                  input_names=['input_ids', \n",
        "                               'segment_ids'],                   # the model's input names\n",
        "                  output_names=['start_logits', \"end_logits\"],   # the model's output names\n",
        "                  dynamic_axes={'input_ids': symbolic_names,              \n",
        "                                'segment_ids' : symbolic_names,\n",
        "                                'start_logits' : symbolic_names, \n",
        "                                'end_logits': symbolic_names})   # variable length axes"
      ],
      "outputs": [],
      "execution_count": 111,
      "metadata": {
        "gather": {
          "logged": 1649368035109
        },
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the ONNX model with ONNX Runtime\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "\n",
        "# Create an ONNX Runtime session to run the ONNX model\n",
        "session = onnxruntime.InferenceSession(output_model_path, providers=[\"CPUExecutionProvider\"])  \n",
        "\n",
        "# Create an IO binding object from the session so that the PyTorch tensors can bound to the ONNX\n",
        "# Runtime inputs and outputs\n",
        "iobinding = session.io_binding()\n"
      ],
      "outputs": [],
      "execution_count": 113,
      "metadata": {
        "gather": {
          "logged": 1649368219911
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Preprocess the question and context into tokenized ids\n",
        "input_ids, segment_ids, tokens = preprocess(question, context)\n",
        "start_logits = torch.Tensor([1, max_seq_len]) \n",
        "end_logits = torch.Tensor([1, max_seq_len])    \n",
        "\n",
        "\n",
        "iobinding.bind_input(\n",
        "    name='input_ids',\n",
        "    device_type=device,\n",
        "    device_id=device_id,\n",
        "    element_type=np.int64,\n",
        "    shape=tuple(input_ids.shape),\n",
        "    buffer_ptr=input_ids.contiguous().data_ptr(),\n",
        "    )\n",
        "\n",
        "iobinding.bind_input(\n",
        "    name='segment_ids',\n",
        "    device_type=device,\n",
        "    device_id=device_id,\n",
        "    element_type=np.int64,\n",
        "    shape=tuple(segment_ids.shape),\n",
        "    buffer_ptr=segment_ids.contiguous().data_ptr(),\n",
        "    )\n",
        "\n",
        "iobinding.bind_output(\n",
        "    name='start_logits',\n",
        "    device_type=device,\n",
        "    device_id=device_id,\n",
        "    element_type=np.float32,\n",
        "    shape=tuple(start_logits.shape),\n",
        "    buffer_ptr=start_logits.contiguous().data_ptr(),\n",
        "    )\n",
        "\n",
        "iobinding.bind_output(\n",
        "    name='end_logits',\n",
        "    device_type=device,\n",
        "    device_id=device_id,\n",
        "    element_type=np.float32,\n",
        "    shape=tuple(end_logits.shape),\n",
        "    buffer_ptr=end_logits.contiguous().data_ptr(),\n",
        "    )\n",
        "\n",
        "# Format the inputs for ONNX Runtime\n",
        "#inputs = {\n",
        "#        'input_ids':   [input_ids], \n",
        "#        'segment_ids': [segment_ids]\n",
        "#        }\n",
        "                    \n",
        "outputs = session.run_with_iobinding(iobinding)\n",
        "\n",
        "# Format the outputs for the post processing function\n",
        "outputs_dict = {\n",
        "        'start_logits': start_logits,\n",
        "        'end_logits': end_logits\n",
        "}\n",
        "\n",
        "# Post process the output of the model into an answer (or an error if the question could not be answered)\n",
        "results = postprocess(tokens, outputs_dict)\n",
        "results\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2022-04-07 21:51:21.648768776 [E:onnxruntime:, sequential_executor.cc:364 Execute] Non-zero status code returned while running Squeeze node. Name:'Squeeze_2313' Status Message: /onnxruntime_src/onnxruntime/core/framework/execution_frame.cc:153 onnxruntime::common::Status onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(int, int, const onnxruntime::TensorShape*, OrtValue*&, const onnxruntime::Node&) shape && tensor.Shape() == *shape was false. OrtValue shape verification failed. Current shape:{2} Requested shape:{1,19}\n\n"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error in execution: Non-zero status code returned while running Squeeze node. Name:'Squeeze_2313' Status Message: /onnxruntime_src/onnxruntime/core/framework/execution_frame.cc:153 onnxruntime::common::Status onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(int, int, const onnxruntime::TensorShape*, OrtValue*&, const onnxruntime::Node&) shape && tensor.Shape() == *shape was false. OrtValue shape verification failed. Current shape:{2} Requested shape:{1,19}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_11836/3104151509.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#        }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_iobinding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miobinding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Format the outputs for the post processing function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/gpu/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun_with_iobinding\u001b[0;34m(self, iobinding, run_options)\u001b[0m\n\u001b[1;32m    274\u001b[0m          \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunOptions\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_iobinding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miobinding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iobinding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error in execution: Non-zero status code returned while running Squeeze node. Name:'Squeeze_2313' Status Message: /onnxruntime_src/onnxruntime/core/framework/execution_frame.cc:153 onnxruntime::common::Status onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(int, int, const onnxruntime::TensorShape*, OrtValue*&, const onnxruntime::Node&) shape && tensor.Shape() == *shape was false. OrtValue shape verification failed. Current shape:{2} Requested shape:{1,19}\n"
          ]
        }
      ],
      "execution_count": 114,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649368282144
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape\r\n",
        "segment_ids.shape\r\n",
        "start_logits.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 68,
          "data": {
            "text/plain": "torch.Size([1])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 68,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649356523983
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy model with ONNX Runtime through AzureML\n",
        "\n",
        "Now that we have prepared ONNX Bert model, we can deploy it using Azure ML and the ONNX Runtime.\n",
        "\n",
        "1. **Register our model** in our Azure Machine Learning workspace\n",
        "2. **Write a scoring file** to evaluate our model with ONNX Runtime\n",
        "3. **Write environment file** for our Docker container image.\n",
        "4. **Deploy to the web** using an AzureML endpoint \n",
        "5. **Classify sample text input** so we can explore inference with our endpoint.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check your environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "import onnxruntime\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(\"Transformers version: \", transformers.__version__)\n",
        "torch_version = torch.__version__\n",
        "print(\"Torch (ONNX exporter) version: \", torch_version)\n",
        "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
        "print(\"ONNX Runtime version: \", onnxruntime.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Transformers version:  4.17.0\nTorch (ONNX exporter) version:  1.10.0\nAzure SDK version: 1.40.0\nONNX Runtime version:  1.11.0\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1649107915721
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load your Azure ML workspace\n",
        "\n",
        "We begin by instantiating a workspace object from the existing workspace created earlier in the configuration notebook."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.location, ws.resource_group, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ort_training_dev\naustraliaeast\nonnx_training\n"
        }
      ],
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1649108263655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register your model with Azure ML\n",
        "\n",
        "Now we upload the model and register it in the workspace.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path = output_model_path,          # Name of the registered model in your workspace.\n",
        "                       model_name = model_name,                 # Local ONNX model to upload and register as a model\n",
        "                       model_framework=Model.Framework.ONNX ,   # Framework used to create the model.\n",
        "                       model_framework_version=torch_version,   # Version of ONNX used to create the model.\n",
        "                       tags = {\"onnx\": \"demo\"},\n",
        "                       description = \"HuggingFace Bert model fine-tuned with SQuAd and exported from PyTorch\",\n",
        "                       workspace = ws)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model bert-large-uncased-whole-word-masking-finetuned-squad\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1648686461468
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Displaying your registered models\n",
        "\n",
        "You can list out all the models that you have registered in this workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "models = ws.models\n",
        "for name, m in models.items():\n",
        "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)\n",
        "    \n",
        "#     # If you'd like to delete the models from workspace\n",
        "#     model_to_delete = Model(ws, name)\n",
        "#     model_to_delete.delete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: hf-gpt2.onnx \tVersion: 1 \tDescription: ONNX version of base HuggingFace GPT-2 {}\nName: hf-gpt2.pt \tVersion: 1 \tDescription: GPT-2 model saved from pre-trained HuggingFace {}\nName: pytorch-hf-gpt-onnx-int8 \tVersion: 1 \tDescription: None {}\nName: pytorch-hf-gpt2-wikitext103 \tVersion: 1 \tDescription: None {}\nName: pt-ort-hf-gpt2-wt103-full \tVersion: 1 \tDescription: HuggingFace GPT-2 fine-tuned with PyTorch ORT using Wikitext103 {}\nName: sample-densenet-onnx-model \tVersion: 1 \tDescription: None {}\nName: bert-large-uncased-whole-word-masking-finetuned-squad \tVersion: 1 \tDescription: HuggingFace Bert model fine-tuned with SQuAd and exported from PyTorch {'onnx': 'demo'}\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1648686965537
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy the model \n",
        "\n",
        "We are now going to deploy our ONNX model on Azure ML using ONNX Runtime.\n",
        "\n",
        "Firstly we will test the deployment using an Azure Container Instance, then deploy the model for production using an Azure ML endpoint.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring (prediction) code\n",
        "\n",
        "We begin by writing a `score.py` file that performs the prediction.\n",
        "\n",
        "The `init()` function is called at startup, performing the one-off operations such as creating the tokenizer and the ONNX Runtime session.\n",
        "\n",
        "The `run()` function is called when we run the model using the Azure ML web service.\n",
        "Add neccessary `preprocess()` and `postprocess()` steps.\n",
        "\n",
        "The following score.py file assumes the inputs will be in the format of the example above. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile score.py\n",
        "import os\n",
        "import collections\n",
        "import json\n",
        "import time\n",
        "import numpy as np    # we're going to use numpy to process input and output data\n",
        "import onnxruntime    # to inference ONNX models, we use the ONNX Runtime\n",
        "from transformers import BasicTokenizer, BertTokenizer\n",
        "from azureml.core.model import Model\n",
        "\n",
        "def init():\n",
        "    global session, tokenizer\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
        "\n",
        "    # use AZUREML_MODEL_DIR to get your deployed model(s). If multiple models are deployed, \n",
        "    # model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), '$MODEL_NAME/$VERSION/$MODEL_FILE_NAME')\n",
        "    # Use the local directory if the environment is not set (for local testing)\n",
        "    model_dir = os.getenv('AZUREML_MODEL_DIR')\n",
        "    if model_dir is None:\n",
        "        model_dir = \".\"\n",
        "    model_path = os.path.join(model_dir, output_model_path)\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    \n",
        "    # Set environment variables like OMP_NUM_THREADS for OpenMP to get best performance.\n",
        "    # See https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/bert/notebooks/PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb\n",
        "    sess_options.intra_op_num_threads = 1\n",
        "    \n",
        "    session = onnxruntime.InferenceSession(model_path, sess_options, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
        "    \n",
        "    \n",
        "def preprocess(input_data_json):\n",
        "    \n",
        "    global all_examples, extra_data\n",
        "    \n",
        "    # Model configs. Adjust as needed.\n",
        "    max_seq_length = 128\n",
        "    doc_stride = 128\n",
        "    max_query_length = 64\n",
        "    is_training = False\n",
        "        \n",
        "    #input_ids, input_mask, segment_ids, extra_data = transformers.squad_convert_examples_to_features(input_data_json, tokenizer,\n",
        "    #                                                                            max_seq_length, doc_stride, max_query_length, is_training)\n",
        "\n",
        "    question = input_data_json[\"question\"]\n",
        "    context = input_data_json[\"context\"]\n",
        "    encoding = tokenizer.encode([question, context])\n",
        "    return encoding.input_ids, encoding.attention_mask, encoding.token_type_ids\n",
        "\n",
        "def postprocess(all_results):\n",
        "    # postprocess results\n",
        "    # from run_onnx_squad import write_predictions\n",
        "\n",
        "    n_best_size = 20\n",
        "    max_answer_length = 30\n",
        "    output_dir = 'predictions'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
        "    #output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
        "    # Write the predictions (answers to the questions) in a file.\n",
        "    #write_predictions(all_examples, extra_data, all_results,\n",
        "    #                n_best_size, max_answer_length,\n",
        "    #                True, output_prediction_file, output_nbest_file)\n",
        "    # Retrieve best results from file.\n",
        "    result = {}\n",
        "    with open(output_prediction_file, \"r\") as f:\n",
        "        result = json.load(f)\n",
        "    return result\n",
        "\n",
        "def run(input_data_json):\n",
        "    try:\n",
        "        # load in our data\n",
        "        input_ids, input_mask, segment_ids = preprocess(input_data_json)\n",
        "        print(input_ids, input_mask, segment_ids)\n",
        "        RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "        print(\"RawResults: \", RawResult)\n",
        "        \n",
        "        n = len(input_ids)\n",
        "        bs = 1\n",
        "        all_results = []\n",
        "        start = time.time()\n",
        "        for idx in range(0, n):\n",
        "            # this is using batch_size=1\n",
        "            # feed the input data as int64\n",
        "            data = {\n",
        "                    \"segment_ids\": segment_ids[idx:idx+bs],\n",
        "                    \"input_ids\": input_ids[idx:idx+bs],\n",
        "                    \"input_mask\": input_mask[idx:idx+bs]\n",
        "                    }\n",
        "            result = session.run([\"start\", \"end\"], data)\n",
        "            print(\"result: \", result)\n",
        "            in_batch = result[0].shape[0]\n",
        "            start_logits = [float(x) for x in result[1][0].flat]\n",
        "            end_logits = [float(x) for x in result[0][0].flat]\n",
        "            for i in range(0, in_batch):\n",
        "                unique_id = len(all_results)\n",
        "                all_results.append(RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits))\n",
        "                \n",
        "        end = time.time()\n",
        "        print(\"total time: {}sec, {}sec per item\".format(end - start, (end - start) / len(all_results)))\n",
        "        return {\"result\": postprocess(all_results),\n",
        "                \"total_time\": end - start, \n",
        "               \"time_per_item\": (end - start) / len(all_results)}\n",
        "    except Exception as e:\n",
        "        result = str(e)\n",
        "        return {\"error\": result}\n",
        "\n",
        "def main():\n",
        "    print(\"Hello World!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    init()\n",
        "    outputs = run()\n",
        "    print(outputs)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting score.py\n"
        }
      ],
      "execution_count": 84,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the score.py locally\n",
        "%run -i score.py \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "loading file https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt from cache at /home/azureuser/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/bert-large-uncased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/bert-large-uncased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/bert-large-uncased/resolve/main/tokenizer_config.json from cache at /home/azureuser/.cache/huggingface/transformers/300ecd79785b4602752c0085f8a89c3f0232ef367eda291c79a5600f3778b677.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nloading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/azureuser/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\nModel config BertConfig {\n  \"_name_or_path\": \"bert-large-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.17.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n"
        }
      ],
      "execution_count": 85,
      "metadata": {
        "gather": {
          "logged": 1649134112978
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies\n",
        "\n",
        "We create a YAML file that specifies the dependencies of the inference application"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "\n",
        "myenv = CondaDependencies.create(pip_packages=[\"numpy\",\"onnxruntime\",\"transformers\", \"torch\", \"azureml-core\", \"azureml-defaults\"])\n",
        "\n",
        "with open(\"myenv.yml\",\"w\") as f:\n",
        "    f.write(myenv.serialize_to_string())"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1649106879458
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're all set! Let's get our model chugging.\n",
        "\n",
        "## Deploy Model as Webservice on Azure Container Instance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell will likely take a few minutes to run as well."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "from azureml.core.webservice import Webservice\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
        "\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
        "                                               memory_gb = 4, \n",
        "                                               tags = {'demo': 'onnx'}, \n",
        "                                               description = 'Web service for Bert-squad-large-uncased ONNX model')\n",
        "\n",
        "# ACI deployment names must be 32 characters or less\n",
        "aci_service_name = model_name[:28] + '-' + str(randint(0,100))\n",
        "print(\"ACI service name: \", aci_service_name)\n",
        "\n",
        "aci_service = Model.deploy(ws, \n",
        "                           aci_service_name, \n",
        "                           [model], \n",
        "                           inference_config, \n",
        "                           aciconfig)\n",
        "\n",
        "aci_service.wait_for_deployment(True)\n",
        "print(\"ACI service state: \", aci_service.state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ACI service name:  bert-large-uncased-whole-wor-51\nTips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\nRunning\n2022-04-04 21:15:56+00:00 Creating Container Registry if not exists.\n2022-04-04 21:15:56+00:00 Registering the environment.\n2022-04-04 21:15:58+00:00 Building image..\n2022-04-04 21:21:57+00:00 Generating deployment configuration.\n2022-04-04 21:22:00+00:00 Submitting deployment to compute..\n2022-04-04 21:22:15+00:00 Checking the status of deployment bert-large-uncased-whole-wor-51..\n2022-04-04 21:25:06+00:00 Checking the status of inference endpoint bert-large-uncased-whole-wor-51."
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: bda42140-b82e-4509-bc35-e83c04abd226\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"AciDeploymentFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\",\n  \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\"\n    },\n    {\n      \"code\": \"AciDeploymentFailed\",\n      \"message\": \"Your container application crashed. Please follow the steps to debug:\n\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\n\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\n\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\n\"RestartCount\": 3\n\"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n\"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2022-04-04T21:26:36.546Z\",\"exitCode\":111,\"finishTime\":\"2022-04-04T21:26:46.957Z\",\"detailStatus\":\"Error\"}\n\"Events\":\n{\"count\":1,\"firstTimestamp\":\"2022-04-04T21:22:19Z\",\"lastTimestamp\":\"2022-04-04T21:22:19Z\",\"name\":\"Pulling\",\"message\":\"pulling image \"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\"\",\"type\":\"Normal\"}\n{\"count\":1,\"firstTimestamp\":\"2022-04-04T21:24:00Z\",\"lastTimestamp\":\"2022-04-04T21:24:00Z\",\"name\":\"Pulled\",\"message\":\"Successfully pulled image \"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\"\",\"type\":\"Normal\"}\n{\"count\":4,\"firstTimestamp\":\"2022-04-04T21:25:00Z\",\"lastTimestamp\":\"2022-04-04T21:26:36Z\",\"name\":\"Started\",\"message\":\"Started container\",\"type\":\"Normal\"}\n{\"count\":3,\"firstTimestamp\":\"2022-04-04T21:25:10Z\",\"lastTimestamp\":\"2022-04-04T21:26:04Z\",\"name\":\"Killing\",\"message\":\"Killing container with id 7eb73b62601f1ce560c3785b23ca7b1e5cbfd6ed6f24d2c75370a994a388c0d4.\",\"type\":\"Normal\"}\n\"\n    }\n  ]\n}\n\n"
        },
        {
          "output_type": "error",
          "ename": "WebserviceException",
          "evalue": "WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: bda42140-b82e-4509-bc35-e83c04abd226\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"AciDeploymentFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\",\n  \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\"\n    },\n    {\n      \"code\": \"AciDeploymentFailed\",\n      \"message\": \"Your container application crashed. Please follow the steps to debug:\n\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\n\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\n\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\n\"RestartCount\": 3\n\"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n\"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2022-04-04T21:26:36.546Z\",\"exitCode\":111,\"finishTime\":\"2022-04-04T21:26:46.957Z\",\"detailStatus\":\"Error\"}\n\"Events\":\n{\"count\":1,\"firstTimestamp\":\"2022-04-04T21:22:19Z\",\"lastTimestamp\":\"2022-04-04T21:22:19Z\",\"name\":\"Pulling\",\"message\":\"pulling image \"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\"\",\"type\":\"Normal\"}\n{\"count\":1,\"firstTimestamp\":\"2022-04-04T21:24:00Z\",\"lastTimestamp\":\"2022-04-04T21:24:00Z\",\"name\":\"Pulled\",\"message\":\"Successfully pulled image \"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\"\",\"type\":\"Normal\"}\n{\"count\":4,\"firstTimestamp\":\"2022-04-04T21:25:00Z\",\"lastTimestamp\":\"2022-04-04T21:26:36Z\",\"name\":\"Started\",\"message\":\"Started container\",\"type\":\"Normal\"}\n{\"count\":3,\"firstTimestamp\":\"2022-04-04T21:25:10Z\",\"lastTimestamp\":\"2022-04-04T21:26:04Z\",\"name\":\"Killing\",\"message\":\"Killing container with id 7eb73b62601f1ce560c3785b23ca7b1e5cbfd6ed6f24d2c75370a994a388c0d4.\",\"type\":\"Normal\"}\n\"\n    }\n  ]\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: bda42140-b82e-4509-bc35-e83c04abd226\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\"\\n    },\\n    {\\n      \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n      \\\"message\\\": \\\"Your container application crashed. Please follow the steps to debug:\\n\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\\n\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\\n\\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\n\\\"RestartCount\\\": 3\\n\\\"CurrentState\\\": {\\\"state\\\":\\\"Waiting\\\",\\\"startTime\\\":null,\\\"exitCode\\\":null,\\\"finishTime\\\":null,\\\"detailStatus\\\":\\\"CrashLoopBackOff: Back-off restarting failed\\\"}\\n\\\"PreviousState\\\": {\\\"state\\\":\\\"Terminated\\\",\\\"startTime\\\":\\\"2022-04-04T21:26:36.546Z\\\",\\\"exitCode\\\":111,\\\"finishTime\\\":\\\"2022-04-04T21:26:46.957Z\\\",\\\"detailStatus\\\":\\\"Error\\\"}\\n\\\"Events\\\":\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-04-04T21:22:19Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:22:19Z\\\",\\\"name\\\":\\\"Pulling\\\",\\\"message\\\":\\\"pulling image \\\"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-04-04T21:24:00Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:24:00Z\\\",\\\"name\\\":\\\"Pulled\\\",\\\"message\\\":\\\"Successfully pulled image \\\"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":4,\\\"firstTimestamp\\\":\\\"2022-04-04T21:25:00Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:26:36Z\\\",\\\"name\\\":\\\"Started\\\",\\\"message\\\":\\\"Started container\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":3,\\\"firstTimestamp\\\":\\\"2022-04-04T21:25:10Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:26:04Z\\\",\\\"name\\\":\\\"Killing\\\",\\\"message\\\":\\\"Killing container with id 7eb73b62601f1ce560c3785b23ca7b1e5cbfd6ed6f24d2c75370a994a388c0d4.\\\",\\\"type\\\":\\\"Normal\\\"}\\n\\\"\\n    }\\n  ]\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_24478/3144885365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                            aciconfig)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0maci_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ACI service state: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maci_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/gpu/lib/python3.9/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output, timeout_sec)\u001b[0m\n\u001b[1;32m    917\u001b[0m                     \u001b[0mlogs_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Current sub-operation type not known, more logs unavailable.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m                 raise WebserviceException('Service deployment polling reached non-successful terminal state, current '\n\u001b[0m\u001b[1;32m    920\u001b[0m                                           \u001b[0;34m'service state: {}\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                                           \u001b[0;34m'Operation ID: {}\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: bda42140-b82e-4509-bc35-e83c04abd226\nMore information can be found using '.get_logs()'\nError:\n{\n  \"code\": \"AciDeploymentFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\",\n  \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\"\n    },\n    {\n      \"code\": \"AciDeploymentFailed\",\n      \"message\": \"Your container application crashed. Please follow the steps to debug:\n\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\n\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\n\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\n\"RestartCount\": 3\n\"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n\"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2022-04-04T21:26:36.546Z\",\"exitCode\":111,\"finishTime\":\"2022-04-04T21:26:46.957Z\",\"detailStatus\":\"Error\"}\n\"Events\":\n{\"count\":1,\"firstTimestamp\":\"2022-04-04T21:22:19Z\",\"lastTimestamp\":\"2022-04-04T21:22:19Z\",\"name\":\"Pulling\",\"message\":\"pulling image \"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\"\",\"type\":\"Normal\"}\n{\"count\":1,\"firstTimestamp\":\"2022-04-04T21:24:00Z\",\"lastTimestamp\":\"2022-04-04T21:24:00Z\",\"name\":\"Pulled\",\"message\":\"Successfully pulled image \"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\"\",\"type\":\"Normal\"}\n{\"count\":4,\"firstTimestamp\":\"2022-04-04T21:25:00Z\",\"lastTimestamp\":\"2022-04-04T21:26:36Z\",\"name\":\"Started\",\"message\":\"Started container\",\"type\":\"Normal\"}\n{\"count\":3,\"firstTimestamp\":\"2022-04-04T21:25:10Z\",\"lastTimestamp\":\"2022-04-04T21:26:04Z\",\"name\":\"Killing\",\"message\":\"Killing container with id 7eb73b62601f1ce560c3785b23ca7b1e5cbfd6ed6f24d2c75370a994a388c0d4.\",\"type\":\"Normal\"}\n\"\n    }\n  ]\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: bda42140-b82e-4509-bc35-e83c04abd226\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: bert-large-uncased-whole-wor-51. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838 locally. Please refer to https://aka.ms/debugimage#service-launch-fails for more information.\\\"\\n    },\\n    {\\n      \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n      \\\"message\\\": \\\"Your container application crashed. Please follow the steps to debug:\\n\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.\\n\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https://aka.ms/debugimage#debug-locally for more information.\\n\\t3. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\n\\\"RestartCount\\\": 3\\n\\\"CurrentState\\\": {\\\"state\\\":\\\"Waiting\\\",\\\"startTime\\\":null,\\\"exitCode\\\":null,\\\"finishTime\\\":null,\\\"detailStatus\\\":\\\"CrashLoopBackOff: Back-off restarting failed\\\"}\\n\\\"PreviousState\\\": {\\\"state\\\":\\\"Terminated\\\",\\\"startTime\\\":\\\"2022-04-04T21:26:36.546Z\\\",\\\"exitCode\\\":111,\\\"finishTime\\\":\\\"2022-04-04T21:26:46.957Z\\\",\\\"detailStatus\\\":\\\"Error\\\"}\\n\\\"Events\\\":\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-04-04T21:22:19Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:22:19Z\\\",\\\"name\\\":\\\"Pulling\\\",\\\"message\\\":\\\"pulling image \\\"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-04-04T21:24:00Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:24:00Z\\\",\\\"name\\\":\\\"Pulled\\\",\\\"message\\\":\\\"Successfully pulled image \\\"orttrainingdf7604408.azurecr.io/azureml/azureml_c24ea65edf5165e43fa82547f30f3838@sha256:626ea2a178599e0dbcdddc5626dc2a532b6666adcda17a5b10e95f35bf5971ab\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":4,\\\"firstTimestamp\\\":\\\"2022-04-04T21:25:00Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:26:36Z\\\",\\\"name\\\":\\\"Started\\\",\\\"message\\\":\\\"Started container\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":3,\\\"firstTimestamp\\\":\\\"2022-04-04T21:25:10Z\\\",\\\"lastTimestamp\\\":\\\"2022-04-04T21:26:04Z\\\",\\\"name\\\":\\\"Killing\\\",\\\"message\\\":\\\"Killing container with id 7eb73b62601f1ce560c3785b23ca7b1e5cbfd6ed6f24d2c75370a994a388c0d4.\\\",\\\"type\\\":\\\"Normal\\\"}\\n\\\"\\n    }\\n  ]\\n}\"\n    }\n}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nFailed\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1649354615535
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case the deployment fails, you can check the logs. Make sure to delete your aci_service before trying again."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if aci_service.state != 'Healthy':\n",
        "    # run this command for debugging.\n",
        "    print(aci_service.get_logs())\n",
        "    aci_service.delete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "None\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1649106182117
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Success!\n",
        "\n",
        "If you've made it this far, you've deployed a working web service that does image classification using an ONNX model. You can get the URL for the webservice with the code below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(aci_service.scoring_uri)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "None\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1649105811536
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.5 - Inference Bert Model using our WebService\n",
        "\n",
        "**Input**: Context paragraph and questions as formatted in `inputs.json`\n",
        "\n",
        "**Task**: For each question about the context paragraph, the model predicts a start and an end token from the paragraph that most likely answers the questions.\n",
        "\n",
        "**Output**: The best answer for each question."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the inputs from step 2.2\n",
        "print(\"========= INPUT DATA =========\")\n",
        "print(json.dumps(inputs_json, indent=2))\n",
        "azure_result = aci_service.run(json.dumps(inputs_json))\n",
        "print(\"\\n\")\n",
        "print(\"========= RESULT =========\")\n",
        "print(json.dumps(azure_result, indent=2))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "res = azure_result['result']\n",
        "inference_time = np.round(azure_result['total_time'] * 1000, 2)\n",
        "time_per_item = np.round(azure_result['time_per_item'] * 1000, 2)\n",
        "\n",
        "print('========================================')\n",
        "print('Final predictions are: ')\n",
        "for key in res:\n",
        "    print(\"Question: \", inputs_json['data'][0]['paragraphs'][0]['qas'][int(key) - 1]['question'])\n",
        "    print(\"Best Answer: \", res[key])\n",
        "    print()\n",
        "\n",
        "print('========================================')\n",
        "print('Inference time: ' + str(inference_time) + \" ms\")\n",
        "print('Average inference time for each question: ' + str(time_per_item) + \" ms\")\n",
        "print('========================================')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are eventually done using the web service, remember to delete it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "aci_service.delete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "gpu-kernel"
    },
    "kernelspec": {
      "display_name": "gpu-kernel",
      "language": "python",
      "name": "gpu-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}